## Application Proposal

In the application part, I will implement at least two iterative machine learning algorithm: `Logistic Regression` and `K-Means` with gradient descent method.  `Logistic Regression` will be used as demonstration at Dec 20, 2018, the test report and addition features will also based on this algorithm for demo.

I will try my best to implement other two algorithms: `Gradient Boosting Decision Tree` and `Latent Dirichlet allocation` if time is allowed.

## Timeline

|Algorithm Name|Requirement|Finish Date|Dataset|
|---|---|---|---|
|Logistic Regression|Mandatory|Nov 23, 2018|jasper/kdd12|
|K-Means|Mandatory|Nov 23, 2018|jasper/SUSY|
|Gradient Boosting Decision Tree|Optional|Dec 20, 2018|Not Decided|
|Latent Dirichlet allocation|Optional|Dec 20, 2018|Not Decided|

## Appendix

### (Mandatory) Logistic Regression

`Logistic regression` is used to predict a discrete outcome based on variables which may be discrete, continuous or mixed. Thus, when the dependent variable has two or more discrete outcomes, logistic regression is a commonly used technique. The outcome could be in the form of Yes / No, 1 / 0, True / False, High/Low, given a set of independent variables.


### (Mandatory) K-Means

`K-Means` clustering is a method of classifying/grouping items into k groups (where k is the number of pre-chosen groups). The grouping is done by minimizing the sum of squared distances (Euclidean distances) between items and the corresponding centroid.

### (Optional) Gradient Boosting Decision Tree

`Gradient Boosting Decision Tree`, also called MART (Multiple Additive Regression Tree), is an iterative decision tree algorithm composed of multiple decision trees. The composition of all trees are added up to make the final answer. It was considered as a generalization algorithm with SVM at the beginning of its introduction.

### (Optional) Latent Dirichlet allocation

`Latent Dirichlet Allocation` is an example of topic model and is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions.

